# Model Hyperparameters Configuration

random_forest:
  n_estimators: 100
  max_depth: 20
  min_samples_split: 5
  min_samples_leaf: 2
  max_features: "sqrt"
  random_state: 42
  n_jobs: -1

gradient_boosting:
  n_estimators: 100
  learning_rate: 0.1
  max_depth: 5
  min_samples_split: 5
  min_samples_leaf: 2
  subsample: 0.8
  random_state: 42

xgboost:
  n_estimators: 100
  learning_rate: 0.1
  max_depth: 6
  min_child_weight: 1
  subsample: 0.8
  colsample_bytree: 0.8
  gamma: 0
  random_state: 42

linear_regression:
  fit_intercept: true
  normalize: false

ridge_regression:
  alpha: 1.0
  fit_intercept: true
  random_state: 42

lasso_regression:
  alpha: 1.0
  fit_intercept: true
  random_state: 42

#neural_network:
#  hidden_layers: [128, 64, 32]
#  activation: "relu"
#  learning_rate: 0.001
#  epochs: 100
#  batch_size: 32
#  validation_split: 0.2
#  early_stopping:
#    patience: 10
#    restore_best_weights: true

# Hyperparameter tuning
hyperparameter_tuning:
  enabled: false
  method: "random_search"  # Options: random_search, grid_search, optuna
  n_trials: 50
  cv_folds: 5
  
  random_forest_grid:
    n_estimators: [50, 100, 200]
    max_depth: [10, 20, 30, null]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2, 4]